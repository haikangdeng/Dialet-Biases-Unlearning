---
seed: 42

# dataset: ChristophSchuhmann/improved_aesthetics_6.5plus
# dataset_split: train
# dialect_file: Indian_English_Simple.csv
# dialect_file: /data2/haikang/projects/cloned/Dialet-Biases-Unlearning/train_val_test/4-1-1/basic/sge/train.csv
dialect_file_folder: /data2/haikang/projects/cloned/Dialet-Biases-Unlearning/data/train_val_test/4-1-1/basic

# tokenizer: openai/clip-vit-large-patch14
# text_encoder: openai/clip-vit-large-patch14
stable_diffusion_model: runwayml/stable-diffusion-v1-5
hf_token: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

optimizer:  # specify the optimizer and its parameters from torch.optim for training.
  AdamW:
    lr: 0.0001
    betas: [0.9, 0.999]
    eps: 1.0e-08
    weight_decay: 0.0

lr_scheduler: # option to provide a learning rate scheduler from torch.optim.
  MultiStepLR:
    milestones: [400]
    gamma: 0.1

# injection:
#   homoglyph_count: 1
#   poisoned_samples_per_step: 128
#   homoglyphs:
#     - homoglyph: ο
#       replaced_character: o
#     - homoglyph: о
#       replaced_character: o
#     - homoglyph: ه
#       replaced_character: o
#     - homoglyph: ㅇ
#       replaced_character: o
#     - homoglyph: ọ
#       replaced_character: o


training: # select the training parameters.
  epochs: 10
  loss_weight: 1.0
  num_steps: 100
  clean_batch_size: 32
  num_threads: 16
  dataloader_num_workers: 8
  save_path: models
  loss_fkt: SimilarityLoss
  alpha_sae_reg: 1.0
  beta_dialect_reg: 0.1
  # gamma_kl_weight: 1.0
 
evaluation: # select parameters for evaluation metrics
  # caption_file: metrics/captions_10000_o.txt
  batch_size: 256
  log_samples: false

rtpt: # state RTPT details. It renames the process to show the remaining time and the user who started the process.
  experiment_name: Dialect Unlearning
  name_initials: HD

wandb: # options for WandB logging.
  enable_logging: true # Set to true to activate the logging.
  args: # arguments for wandb.init call. See https://docs.wandb.ai/ref/python/init for a complete overview.
    project: Dialect Unlearning
    name: dialect_unlearning
    save_code: true